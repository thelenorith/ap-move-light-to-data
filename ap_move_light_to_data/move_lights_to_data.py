"""
Move light frames to data directory when calibration frames are available.

Generated By: Claude Code (Claude Sonnet 4.5)
"""

import argparse
import logging
import os
import re
import shutil
from pathlib import Path
from typing import List, Dict, Any, Optional

import ap_common
from ap_common import setup_logging, progress_iter
from ap_common.progress import ProgressTracker

from . import config
from .matching import (
    get_light_frames,
    find_all_light_directories,
    check_calibration_for_light,
    is_file_inside_tree,
)

EXIT_SUCCESS = 0
EXIT_ERROR = 1

logger = logging.getLogger("ap_move_light_to_data.move_lights_to_data")

# Set default description width for aligned progress bars
ProgressTracker.set_default_desc_width(20)


def build_search_dirs(directory: str, source_dir: str) -> List[str]:
    """
    Build list of directories to search for calibration.

    Searches current directory and parents up to source.

    Args:
        directory: Starting directory
        source_dir: Source root (boundary, not included in search)

    Returns:
        List of directory paths ordered from child to parent
    """
    search_dirs = []
    current = Path(directory).resolve()
    source = Path(ap_common.replace_env_vars(source_dir)).resolve()

    while current != source and current.parent != current:
        search_dirs.append(str(current))
        current = current.parent
        if current == source:
            break

    return search_dirs


def is_group_complete_and_self_contained(
    group_path: str,
    source_dir: str,
    scale_darks: bool,
    debug: bool,
    quiet: bool,
    metadata_cache: Optional[Dict[str, Dict[str, Any]]] = None,
) -> Dict[str, Any]:
    """
    Check if a directory group is complete (all lights have calibration)
    and self-contained (all calibration is inside the group).

    Args:
        group_path: Directory group to evaluate
        source_dir: Source root directory
        scale_darks: Allow shorter darks with bias frames
        debug: Enable debug output
        quiet: Suppress progress output
        metadata_cache: Optional pre-loaded metadata dict

    Returns:
        Dict with:
            - is_complete: bool (all lights have calibration)
            - is_self_contained: bool (all calibration inside group)
            - can_move: bool (complete AND self-contained)
            - light_directories: List[str] (all light dirs in group)
            - calibration_files: Set[str] (all required calibration file paths)
            - incomplete_dirs: List[str] (light dirs missing calibration)
    """
    resolved_group_path = Path(group_path).resolve()

    result = {
        "is_complete": False,
        "is_self_contained": False,
        "can_move": False,
        "light_directories": [],
        "calibration_files": set(),
        "incomplete_dirs": [],
    }

    # Find all light directories in this group
    light_dirs = find_all_light_directories(
        root_dir=str(resolved_group_path),
        metadata_cache=metadata_cache if metadata_cache is not None else {},
        debug=debug,
    )
    result["light_directories"] = light_dirs

    if not light_dirs:
        return result

    # Check calibration for each light directory
    all_calibration_files = set()
    incomplete_dirs = []

    for light_dir in light_dirs:
        # Get one light frame as reference
        lights = get_light_frames(
            directory=light_dir,
            metadata_cache=metadata_cache if metadata_cache is not None else {},
            debug=debug,
        )
        if not lights:
            continue

        light_metadata = next(iter(lights.values()))

        # Build search directories (light dir + parents up to source)
        search_dirs = build_search_dirs(light_dir, source_dir)

        # Check calibration
        cal_status = check_calibration_for_light(
            light_metadata=light_metadata,
            search_dirs=search_dirs,
            metadata_cache=metadata_cache if metadata_cache is not None else {},
            scale_darks=scale_darks,
            debug=debug,
            quiet=quiet,
        )

        if not cal_status["is_complete"]:
            incomplete_dirs.append((light_dir, cal_status["missing"]))
        else:
            # Collect all calibration file paths
            all_calibration_files.update(cal_status["matched_darks"])
            all_calibration_files.update(cal_status["matched_flats"])
            all_calibration_files.update(cal_status["matched_bias"])

    result["incomplete_dirs"] = incomplete_dirs
    result["calibration_files"] = all_calibration_files
    result["is_complete"] = len(incomplete_dirs) == 0

    # Check if self-contained (all calibration inside group)
    if result["is_complete"]:
        all_inside = all(
            is_file_inside_tree(cal_file, str(resolved_group_path))
            for cal_file in all_calibration_files
        )
        result["is_self_contained"] = all_inside
        result["can_move"] = all_inside

    return result


def filter_by_pattern(light_dirs: List[str], path_pattern: Optional[str]) -> List[str]:
    """
    Step 2: Filter light directories by pattern.

    Args:
        light_dirs: List of light directory paths
        path_pattern: Optional regex pattern to filter paths

    Returns:
        Filtered list of light directory paths
    """
    if path_pattern:
        filtered = [d for d in light_dirs if re.search(path_pattern, d)]
        logger.debug(
            f"Pattern '{path_pattern}' matched "
            f"{len(filtered)} of {len(light_dirs)} directories"
        )
        return filtered
    return light_dirs


def check_light_directories(
    light_dirs: List[str],
    source_dir: Path,
    scale_darks: bool,
    debug: bool,
    quiet: bool,
    metadata_cache: Optional[Dict[str, Dict[str, Any]]] = None,
) -> Dict[str, Dict[str, Any]]:
    """
    Step 3: Check calibration status for each light directory.

    Args:
        light_dirs: List of light directory paths
        source_dir: Source root directory
        scale_darks: Allow shorter darks with bias frames
        debug: Enable debug output
        quiet: Suppress progress output
        metadata_cache: Optional pre-loaded metadata dict

    Returns:
        Dict mapping light_dir -> calibration status dict
    """
    status_map = {}

    for light_dir in progress_iter(
        light_dirs, desc="Checking calibration", enabled=not quiet
    ):
        # Get one light frame as reference
        lights = get_light_frames(
            directory=light_dir,
            metadata_cache=metadata_cache if metadata_cache is not None else {},
            debug=debug,
        )
        if not lights:
            continue

        light_metadata = next(iter(lights.values()))

        # Build search directories (light dir + parents up to source)
        search_dirs = build_search_dirs(light_dir, str(source_dir))

        # Check calibration
        cal_status = check_calibration_for_light(
            light_metadata=light_metadata,
            search_dirs=search_dirs,
            metadata_cache=metadata_cache if metadata_cache is not None else {},
            scale_darks=scale_darks,
            debug=debug,
            quiet=quiet,
        )

        # Add directory info
        status_map[light_dir] = {
            "is_complete": cal_status["is_complete"],
            "missing": cal_status["missing"],
            "calibration_files": set(
                cal_status["matched_darks"]
                + cal_status["matched_flats"]
                + cal_status["matched_bias"]
            ),
        }

    return status_map


def find_calibration_directories(
    status_map: Dict[str, Dict[str, Any]],
) -> Dict[str, str]:
    """
    Step 4a: Find where calibration exists for each light directory.

    For each light dir, determine which directory contains its calibration files.
    This could be the light dir itself, or any parent directory.

    Args:
        status_map: Dict mapping light_dir -> calibration status with calibration_files

    Returns:
        Dict mapping light_dir -> calibration_directory
    """
    calibration_dirs = {}

    for light_dir, status in status_map.items():
        if not status["calibration_files"]:
            # No calibration files, skip
            continue

        # Find the shallowest directory that contains all calibration files
        light_path = Path(light_dir)
        calibration_paths = [Path(f) for f in status["calibration_files"]]

        # Start with light dir itself, then check parents
        candidate_dirs = [light_path]
        current = light_path.parent
        while current.parent != current:
            candidate_dirs.append(current)
            current = current.parent

        # Find shallowest dir that contains all calibration
        for candidate in candidate_dirs:
            if all(
                is_file_inside_tree(str(cp), str(candidate)) for cp in calibration_paths
            ):
                calibration_dirs[light_dir] = str(candidate)
                break

    return calibration_dirs


def organize_into_movable_groups(
    status_map: Dict[str, Dict[str, Any]], source_dir: Path
) -> Dict[str, Any]:
    """
    Step 4b: Determine which directories can be moved atomically.

    Logic:
    1. Find calibration directory for each complete light dir
    2. Exclude calibration dirs that are parents of ANY incomplete light dir
    3. What remains are directories containing only complete lights

    Args:
        status_map: Dict mapping light_dir -> calibration status
        source_dir: Source root directory

    Returns:
        Dict with 'movable_dirs' and 'incomplete_dirs'
    """
    # Separate complete and incomplete
    complete_lights = {
        ld: status for ld, status in status_map.items() if status["is_complete"]
    }
    incomplete_lights = [
        (ld, status["missing"])
        for ld, status in status_map.items()
        if not status["is_complete"]
    ]

    # Find calibration directories for complete lights
    cal_dirs_map = find_calibration_directories(complete_lights)

    # Get unique calibration directories
    calibration_dirs = set(cal_dirs_map.values())

    # Exclude calibration dirs that are parents of ANY incomplete light
    for incomplete_light, _ in incomplete_lights:
        incomplete_path = Path(incomplete_light)
        # Remove any calibration dir that contains this incomplete light
        to_remove = set()
        for cal_dir in calibration_dirs:
            if is_file_inside_tree(str(incomplete_path), cal_dir):
                to_remove.add(cal_dir)
        calibration_dirs -= to_remove

    # Deduplicate nested calibration directories (keep only parents)
    # If M31 and M31/2024-01-01 are both calibration dirs, only keep M31
    deduplicated_dirs: set[str] = set()
    sorted_dirs = sorted(calibration_dirs, key=lambda x: x.count(os.sep))

    for cal_dir in sorted_dirs:
        # Check if this dir is a child of any already-added parent
        is_child = False
        for parent_dir in deduplicated_dirs:
            if is_file_inside_tree(cal_dir, parent_dir):
                is_child = True
                break
        if not is_child:
            deduplicated_dirs.add(cal_dir)

    # Build movable groups
    movable_groups = []
    for cal_dir in sorted(deduplicated_dirs, key=lambda x: x.count(os.sep)):
        movable_groups.append(
            {
                "path": Path(cal_dir),
                "relative_path": Path(cal_dir).relative_to(source_dir),
            }
        )

    return {"movable_groups": movable_groups, "incomplete_dirs": incomplete_lights}


def collect_all_files_in_groups(
    movable_groups: List[Dict], dest_dir: Path
) -> List[Dict[str, str]]:
    """
    Collect all files across all groups with source and destination paths.

    Args:
        movable_groups: List of group_plan dicts with "path" and "relative_path"
        dest_dir: Destination root directory

    Returns:
        List of dicts with:
            - "source": source file path
            - "dest": destination file path
            - "group": which group this file belongs to (for error reporting)
    """
    files = []
    for group_plan in movable_groups:
        source_group = Path(group_plan["path"])
        for root, dirs, filenames in os.walk(source_group):
            for filename in filenames:
                source_file = Path(root) / filename
                # Calculate relative path within this group
                rel_to_group = source_file.relative_to(source_group)
                # Build destination path
                dest_file = dest_dir / group_plan["relative_path"] / rel_to_group

                files.append(
                    {
                        "source": str(source_file),
                        "dest": str(dest_file),
                        "group": group_plan["relative_path"],
                    }
                )
    return files


def sort_groups_leaf_first(movable_groups: List[Dict]) -> List[Dict]:
    """
    Sort groups in leaf-first order (deepest paths first).

    This ensures child directories are processed before parent directories,
    preventing broken states where calibration exists without lights.

    Args:
        movable_groups: List of group_plan dicts

    Returns:
        Sorted list with deepest paths first
    """
    return sorted(
        movable_groups,
        key=lambda t: str(t["path"]).count(os.sep),
        reverse=True,  # Deepest first
    )


def process_light_directories(
    source_dir: str,
    dest_dir: str,
    path_pattern: str,
    debug: bool = False,
    dry_run: bool = False,
    quiet: bool = False,
    scale_darks: bool = False,
) -> dict:
    """
    Move complete directory groups atomically using discrete steps:
    1. Collect: Find all light directories recursively
    2. Filter: Apply pattern to collected directories
    3. Check: Determine calibration status for each
    4. Organize: Find movable calibration directories (exclude parents of incomplete)
    5. Move: Execute atomic directory moves
    6. Report: Track results and incomplete directories

    Args:
        source_dir: Source directory (e.g., 10_Blink)
        dest_dir: Destination directory (e.g., 20_Data)
        path_pattern: Regex pattern to filter paths
        debug: Enable debug output
        dry_run: Preview without moving
        quiet: Suppress progress output
        scale_darks: Allow shorter darks with bias frames

    Returns:
        Dict with counts: moved, skipped_*, errors
    """
    source_path = Path(ap_common.replace_env_vars(source_dir)).resolve()
    dest_path = Path(ap_common.replace_env_vars(dest_dir)).resolve()

    results = {
        "dir_count": 0,
        "target_count": 0,
        "date_count": 0,
        "filter_count": 0,
        "moved": 0,
        "skipped_no_darks": 0,
        "skipped_no_flats": 0,
        "skipped_no_bias": 0,
        "biases_needed": 0,
        "errors": 0,
    }

    if not source_path.exists():
        logger.error(f"Source directory does not exist: {source_path}")
        return results

    # Phase 0: Load all metadata upfront (single pass)
    logger.info("Loading all metadata from source directory...")
    try:
        metadata_cache = ap_common.get_metadata(
            dirs=[str(source_path)],
            profileFromPath=True,
            patterns=config.SUPPORTED_EXTENSIONS,
            recursive=True,
            # Request union of all properties needed for any frame type matching
            # This ensures ALL files get enriched with actual FITS/XISF headers
            # (not just filename metadata), even if the filename
            # contains some properties
            required_properties=config.ALL_REQUIRED_KEYWORDS,
            debug=debug,
            printStatus=not quiet,
        )
        logger.debug(f"Loaded metadata for {len(metadata_cache):,} files")
    except (OSError, ValueError) as e:
        # If metadata loading fails (e.g., corrupt files), fall back to lazy loading
        logger.warning(f"Failed to load metadata cache: {e}")
        logger.warning("Falling back to lazy metadata loading (will be slower)")
        metadata_cache = None

    # Step 1: COLLECT
    all_light_dirs = find_all_light_directories(
        root_dir=str(source_path),
        metadata_cache=metadata_cache if metadata_cache is not None else {},
        debug=debug,
    )

    if not all_light_dirs:
        logger.warning(f"No light directories found in {source_path}")
        return results

    # Step 2: FILTER
    filtered_light_dirs = filter_by_pattern(all_light_dirs, path_pattern)

    if not filtered_light_dirs:
        logger.warning(f"No light directories matched pattern in {source_path}")
        return results

    results["dir_count"] = len(filtered_light_dirs)

    # Extract targets for metrics
    targets = set()
    for light_dir in filtered_light_dirs:
        try:
            rel = Path(light_dir).relative_to(source_path)
            if rel.parts:
                targets.add(rel.parts[0])
        except ValueError:
            pass
    results["target_count"] = len(targets)

    # Step 3: CHECK
    status_map = check_light_directories(
        filtered_light_dirs, source_path, scale_darks, debug, quiet, metadata_cache
    )

    # Step 4: ORGANIZE
    organized = organize_into_movable_groups(status_map, source_path)
    movable_groups = organized["movable_groups"]
    incomplete_dirs = organized["incomplete_dirs"]

    # Process incomplete dirs for metrics
    for light_dir, missing in incomplete_dirs:
        if "darks" in missing:
            results["skipped_no_darks"] += 1
        if "flats" in missing:
            results["skipped_no_flats"] += 1
        if "bias" in missing:
            results["skipped_no_bias"] += 1
            results["biases_needed"] += 1

    # Step 5: MOVE groups atomically
    # Sort groups in leaf-first order (deepest first)
    # This prevents broken states where calibration exists without lights
    movable_groups_ordered = sort_groups_leaf_first(movable_groups)

    if not dry_run:
        # Phase 1: Copy all files (leaf-first order)
        # Collect all files to copy
        logger.debug("Analyzing files to move...")
        all_files = collect_all_files_in_groups(movable_groups_ordered, dest_path)

        logger.debug(
            f"Copying {len(all_files):,} files "
            f"across {len(movable_groups):,} directories..."
        )

        # Copy with file-level progress
        copy_errors = []
        for file_info in progress_iter(
            all_files,
            desc="Copying files",
            unit="files",
            enabled=not quiet,
        ):
            try:
                ap_common.copy_file(
                    file_info["source"],
                    file_info["dest"],
                    debug=debug,
                    dryrun=dry_run,
                )
            except Exception as e:
                error_msg = f"Failed to copy {file_info['source']}: {e}"
                logger.error(error_msg)
                copy_errors.append(error_msg)
                # Continue copying other files even if one fails

        # Report copy phase results
        if copy_errors:
            logger.error(f"Copy phase had {len(copy_errors)} errors")
            for error in copy_errors[:10]:  # Show first 10 errors
                logger.error(f"  {error}")
            if len(copy_errors) > 10:
                logger.error(f"  ... and {len(copy_errors) - 10} more errors")
            results["errors"] += len(copy_errors)

        # Phase 2: Delete source groups (only if copy succeeded)
        if not copy_errors:
            logger.info(f"Deleting {len(movable_groups):,} source directories...")

            for group_plan in movable_groups_ordered:
                source_group = Path(group_plan["path"])
                try:
                    shutil.rmtree(source_group)
                    results["moved"] += 1
                    logger.debug(f"Deleted source group: {group_plan['relative_path']}")
                except Exception as e:
                    error_msg = f"Failed to delete {group_plan['relative_path']}: {e}"
                    logger.error(error_msg)
                    results["errors"] += 1

            # Clean up empty parent directories
            logger.info("Cleaning up empty directories...")
            ap_common.delete_empty_directories(
                str(source_path),
                dryrun=dry_run,
                printStatus=not quiet,
            )
        else:
            logger.warning("Skipping source deletion due to copy errors")
            logger.warning(
                "Source files remain intact. Fix errors and re-run to complete move."
            )
    else:
        # Dry-run: just count what would be moved
        movable_groups_ordered = sort_groups_leaf_first(movable_groups)
        all_files = collect_all_files_in_groups(movable_groups_ordered, dest_path)
        results["moved"] = len(movable_groups)
        logger.info(
            f"DRY RUN: Would move {len(all_files):,} files "
            f"across {len(movable_groups):,} directories"
        )

    # Step 6: REPORT incomplete directories
    if incomplete_dirs and not quiet:
        print("\nThe following directories are missing calibration:")
        for light_dir, missing in incomplete_dirs:
            try:
                rel = Path(light_dir).relative_to(source_path)
                missing_str = ", ".join(missing)
                print(f"  - {rel} (missing: {missing_str})")
            except ValueError:
                pass

    return results


def print_summary(results: dict, scale_darks: bool = False) -> None:
    """Print summary of processing results."""

    def plural(count: int, singular: str) -> str:
        return f"{count} {singular}{'s' if count != 1 else ''}"

    def status_indicator(present: int, needed: int) -> str:
        return "ok" if present >= needed else "MISSING!"

    print(f"\n{'='*70}")
    print("Summary")
    print(f"{'='*70}")

    dir_count = results["dir_count"]
    print(
        f"Directories: {dir_count} "
        f"({plural(results['target_count'], 'target')}, "
        f"{plural(results['date_count'], 'date')}, "
        f"{plural(results['filter_count'], 'filter')})"
    )

    # Bias, Darks, Flats order (required by tests)
    if scale_darks:
        biases_needed = results["biases_needed"]
        biases_present = biases_needed - results["skipped_no_bias"]
    else:
        biases_present = 0
        biases_needed = 0

    print(
        f"Biases: {biases_present} of {biases_needed} | "
        f"{status_indicator(biases_present, biases_needed)}"
    )
    print(
        f"Darks:  {dir_count - results['skipped_no_darks']} of {dir_count} | "
        f"{status_indicator(dir_count - results['skipped_no_darks'], dir_count)}"
    )
    print(
        f"Flats:  {dir_count - results['skipped_no_flats']} of {dir_count} | "
        f"{status_indicator(dir_count - results['skipped_no_flats'], dir_count)}"
    )

    if results["errors"] > 0:
        print(f"Errors: {results['errors']}")
    print(f"{'='*70}\n")


def main() -> int:
    """Main entry point for CLI."""
    parser = argparse.ArgumentParser(
        description=(
            "Move complete directory groups containing "
            "light frames and calibration atomically."
        )
    )

    parser.add_argument("source_dir", help="source directory containing lights")
    parser.add_argument("dest_dir", help="destination directory for lights")
    parser.add_argument(
        "--debug", "-d", action="store_true", help="Enable debug output"
    )
    parser.add_argument(
        "--dryrun", "-n", action="store_true", help="Preview without moving"
    )
    parser.add_argument(
        "--quiet", "-q", action="store_true", help="Suppress progress output"
    )
    parser.add_argument(
        "--scale-dark",
        action=argparse.BooleanOptionalAction,
        default=False,
        help="scale dark frames using bias compensation (allows shorter exposures). "
        "Default: exact exposure match only",
    )
    parser.add_argument(
        "--path-pattern",
        type=str,
        default=config.DEFAULT_PATH_PATTERN,
        help=(
            "Regex pattern to filter paths "
            f'(default: "{config.DEFAULT_PATH_PATTERN}")'
        ),
    )

    args = parser.parse_args()

    # Setup logging
    setup_logging(name="ap_move_light_to_data", debug=args.debug, quiet=args.quiet)

    # Validate directories
    source_path = Path(ap_common.replace_env_vars(args.source_dir))
    if not source_path.exists():
        print(f"ERROR: Source directory does not exist: {source_path}")
        return EXIT_ERROR
    if not source_path.is_dir():
        print(f"ERROR: Source path is not a directory: {source_path}")
        return EXIT_ERROR

    dest_path = Path(ap_common.replace_env_vars(args.dest_dir))
    if dest_path.exists() and not dest_path.is_dir():
        print(f"ERROR: Destination exists but is not a directory: {dest_path}")
        return EXIT_ERROR

    print(f"Source directory: {args.source_dir}")
    print(f"Destination directory: {args.dest_dir}")

    if args.dryrun:
        print("\n*** DRY RUN - No files will be moved ***\n")

    results = process_light_directories(
        args.source_dir,
        args.dest_dir,
        args.path_pattern,
        args.debug,
        args.dryrun,
        args.quiet,
        args.scale_darks,
    )

    if not args.quiet:
        print_summary(results, scale_darks=args.scale_darks)

    return EXIT_ERROR if results["errors"] > 0 else EXIT_SUCCESS


if __name__ == "__main__":
    import sys

    sys.exit(main())
